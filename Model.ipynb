{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade keras","metadata":{"execution":{"iopub.status.busy":"2023-01-24T21:08:16.750299Z","iopub.execute_input":"2023-01-24T21:08:16.750753Z","iopub.status.idle":"2023-01-24T21:08:32.748160Z","shell.execute_reply.started":"2023-01-24T21:08:16.750720Z","shell.execute_reply":"2023-01-24T21:08:32.746999Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (2.6.0)\nCollecting keras\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.6.0\n    Uninstalling keras-2.6.0:\n      Successfully uninstalled keras-2.6.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires keras<2.7,>=2.6.0, but you have keras 2.11.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.11.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importez les bibliothèques nécessaires\nimport numpy as np\nimport pandas as pd\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-01-24T21:13:47.459789Z","iopub.execute_input":"2023-01-24T21:13:47.460384Z","iopub.status.idle":"2023-01-24T21:13:47.467465Z","shell.execute_reply.started":"2023-01-24T21:13:47.460338Z","shell.execute_reply":"2023-01-24T21:13:47.466283Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#import du jeu de données\n(x_train, y_train), (x_test, y_test) = 'jeux de données'","metadata":{"execution":{"iopub.status.busy":"2023-01-24T21:14:10.024538Z","iopub.execute_input":"2023-01-24T21:14:10.025049Z","iopub.status.idle":"2023-01-24T21:14:10.053104Z","shell.execute_reply.started":"2023-01-24T21:14:10.025014Z","shell.execute_reply":"2023-01-24T21:14:10.051836Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/380571190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import du jeu de données\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"],"ename":"NameError","evalue":"name 'load_data' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Chargement des données d'entraînement et de test\ntrain_data = pd.read_csv(\"train.csv\")\ntest_data = pd.read_csv(\"test.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Séparez les données en entrées et étiquettes\nX = train_data.iloc[:, 1:].values\ny = train_data.iloc[:, 0].values\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Effectuez un split des données d'entraînement et de validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prétraitement des données\nX_train = X_train.reshape(-1, 180, 180, 1)\nX_val = X_val.reshape(-1, 180, 180, 1)\nX_test = test_data.values.reshape(-1, 180, 180, 1)\nX_train = X_train / 255.0\nX_val = X_val / 255.0\nX_test = X_test / 255.0\ny_train = to_categorical(y_train, num_classes=5270)\ny_val = to_categorical(y_val, num_classes=5270)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Génération de données supplémentaires\ndata_generator = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\ndata_generator.fit(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Créez un modèle de réseau neuronal convolutionnel\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(180,180,1)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5270, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compilez le modèle\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Entraînez le modèle\nmodel.fit_generator(data_generator.flow(X_train, y_train, batch_size=32), steps_per_epoch=len(X_train)/32, epochs=10, validation_data=(X_val, y_val))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Évaluez le modèle sur les données de validation\nval_loss, val_acc = model.evaluate(X_val, y_val)\nprint(\"Validation Loss: \", val_loss)\nprint(\"Validation Accuracy: \", val_acc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utilisez le modèle pour prédire les catégories des données de test\ntest_predictions = model.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convertir les prédictions en catégories numériques\ntest_predictions = np.argmax(test_predictions, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Enregistrer les prédictions dans un fichier CSV pour soumission\nsubmission = pd.DataFrame({'Id': test_data.Id, 'Category': test_predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}